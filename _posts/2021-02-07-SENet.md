---
layout: single
title: "[논문리뷰] SENet - Squeeze-and-Excitation Networks"
tage: [ImageRecognition]
comments: true
published: true
categories: ImageRecognition
use_math: true
typora-copy-images-to: ..\images\SENet
---



**[논문리뷰] SENet - Squeeze-and-Excitation Networks**



## **Squeeze-and-Excitation Block**



![image-20210207203338698](/images/SENet/image-20210207203338698.png)



- VGG, GoogLeNet, ResNet 등 네트워크 어떤 곳이라도 바로 붙일 수 있음

  

- Parameters의 증가량에 비해 모델 성능 향상도가 매우 큼. 

  

- 모델 복잡도(Model complexity)와 계산 복잡도(computational burden)이 크게 증가하지 않음

  

- Convolution을 통해 생성된 특성을 채널당 중요도를 고려해서 재보정함



<img src="/images/SENet/image-20210207203607115.png" alt="image-20210207203607115" style="zoom:80%;" />



$F_{tr}$은 단순한 convolution 연산



σ는 시그모이드함수이고, δ는 ReLU 함수

<br/>

<br/>

## **Squeeze : Global Information Embedding**

<br/>

![image-20210207203655485](/images/SENet/image-20210207203655485.png)

<br/>

- WxHxC의 feature map이 1x1xC으로 squeeze 되어 각 채널을 하나의 숫자로 나타냄 (feature을 global하게 표현)

  

- 논문에서는 GAP(Global Average Pooling)을 사용

  

- Local receptive field가 작은 네트워크 하위 부분에서는 중요 정보 추출이 중요함

  

<img src="/images/SENet/image-20210207203753357.png" alt="image-20210207203753357" style="zoom:67%;" />

<br/>

<br/>

## **Excitation: Adaptive Recalibration**

![image-20210207203820106](/images/SENet/image-20210207203820106.png)

- 두 개의 Fully-connected(FC) 층을 더해줘서 각 채널의 상대적 중요도를 알아냄

  

- FC 층들이 bottleneck 구조가 되게 만듦



<img src="C:\Users\1\Documents\GitHub\blackchopin.github.io\images\SENet\image-20210207203854304.png" alt="image-20210207203854304" style="zoom:75%;" />



<br/>

<br/>

## **Bottleneck structure**

<br/>

![image-20210207203931261](/images/SENet/image-20210207203931261.png)

<br/>

- Reduction ratio(r) 을 통해 W1의 노드 수를 줄이고, W2에서 다시 C만큼 증가시킴

  

- hyper-parameter의 개수를 늘이지 않으며, 일반화에 도움을 줌

  

- 특성맵 은 X → U → X ̃



C개의 sc를 uc에 각각 곱해줘서 U를 재보정해줌. 이를 x~라 함

특성맵은 X에서 컨볼루션을 통해 U로, U에서 SE block을 통해 X~로 변환

<br/>

<br/>

## **Apply SE block at Networks**

<br/>

![image-20210207204034615](/images/SENet/image-20210207204034615.png)

<br/>

- SE block은 기존의 CNN모델에 붙여서 사용 가능

  

- Inception module과 skip connection이 있는 네트워크들은 위와 같이 적용

  

- 그 이외의 네트워크(VGGNet)들은 바로 적용

<br/>

<br/>

## 실험 결과

![image-20210207204128349](/images/SENet/image-20210207204128349.png)

![image-20210207204132738](/images/SENet/image-20210207204132738.png)



Original은 논문에서 얻은 결과를, re-implementation은 직접 실험해서 얻은 결과를, SENet은 SE block을 첨가해서 얻은 결과를 나타냄

GFLOPs는 유사하지만, error는 낮아졌다.