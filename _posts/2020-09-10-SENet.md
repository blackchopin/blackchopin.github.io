---
layout: single
title: "[논문리뷰] SENet - Squeeze-and-Excitation Networks"
tage: [ImageRecognition]
comments: true
published: true
categories: ImageRecognition
use_math: true
typora-copy-images-to: ..\images\SENet
---

<br/>

**[논문리뷰] SENet - Squeeze-and-Excitation Networks**

<br/>

## **Squeeze-and-Excitation Block**

<br/>

![image-20210207203338698](/images/SENet/image-20210207203338698.png)

 *빨간 네모로 표시해둔 부분이 Squeeze-and-Excitation Block 이다.*

<br/>

- VGG, GoogLeNet, ResNet 등 네트워크 어떤 곳이라도 바로 붙일 수 있다는 장점이 있다.

- 파라미터의 증가량에 비해 모델 성능 향상도가 매우 커 효율적이다. 

- 모델 복잡도(Model complexity)와 계산 복잡도(computational burden)이 크게 증가하지 않는다.

- **Convolution을 통해 생성된 특성을 채널당 중요도를 고려해서 재보정함** (각 채널의 정보를 하나의 스칼라값으로 통합하므로 이를 channel attention이라고도 한다)

<br/>

<img src="/images/SENet/image-20210207203607115.png" alt="image-20210207203607115" style="zoom:80%;" />

<br/>

$F_{tr}$은 단순한 convolution 연산을 의미한다.

σ는 시그모이드, δ는 ReLU 함수를 의미한다.

<br/>

<br/>

## **Squeeze : Global Information Embedding**

<br/>

![image-20210207203655485](/images/SENet/image-20210207203655485.png)

<br/>

- **WxHxC의 feature map이 1x1xC으로 squeeze 되어 각 채널을 하나의 숫자로 나타내어 진다. (feature을 global하게 표현)**

- 논문에서는 GAP(Global Average Pooling)을 사용하여 channel attention을 수행한다.

- Local receptive field가 작은 네트워크 하위 부분에서는 중요 정보 추출이 중요하기에 위 과정이 효과적이다.

  

<img src="/images/SENet/image-20210207203753357.png" alt="image-20210207203753357" style="zoom:67%;" />

<br/>

<br/>

## **Excitation: Adaptive Recalibration**

<br/>

![image-20210409213246186](/images/SENet/image-20210409213246186.png)

<br/>

- 두 개의 Fully-connected(FC) 층을 더해줘서 **각 채널의 상대적 중요도를 알아냄**

- FC 층들이 병목(bottleneck) 구조가 되게 만듦 (병목구조에 관한 설명은 아래에!)

<br/>

<img src="/images/SENet/image-20210207203854304.png" alt="image-20210207203854304" style="zoom:75%;" />

<br/>

<br/>

## **Bottleneck structure**

<br/>

![image-20210207203931261](/images/SENet/image-20210207203931261.png)

<br/>

- Reduction ratio(r) 을 통해 W1의 노드 수를 줄이고, W2에서 다시 C만큼 증가시킨다.
- hyper-parameter의 개수를 늘이지 않으며, 일반화에 도움을 준다.
- 특성맵 은 X → U → X ̃로 변화한다.

<br/>

C개의 sc를 uc에 각각 곱해줘서 U를 재보정해줌. 이를 x~라 한다.

특성맵은 X에서 컨볼루션을 통해 U로, U에서 SE block을 통해 X~로 변환한다.

<br/>

<br/>

## **Apply SE block at Networks**

<br/>

![image-20210207204034615](/images/SENet/image-20210207204034615.png)

<br/>

- SE block은 기존의 CNN모델에 붙여서 사용 가능하다.

- Inception module과 skip connection이 있는 네트워크들은 위와 같이 적용한다.

- 그 이외의 네트워크(VGGNet)들은 바로 적용한다.

<br/>

<br/>

## 실험 결과

<br/>

![image-20210207204128349](/images/SENet/image-20210207204128349.png)

<br/>

![image-20210207204132738](/images/SENet/image-20210207204132738.png)

<br/>

Original은 논문에서 얻은 결과를, re-implementation은 논문의 저자들이 직접 실험해서 얻은 결과를, SENet은 SE block을 첨가해서 얻은 결과를 나타낸다.

GFLOPs는 유사하지만, error는 낮아졌다. -> 효율적 구조 의미!